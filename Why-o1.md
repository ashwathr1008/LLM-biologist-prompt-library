# The Strengths and Weaknesses of ChatGPT's o-1 Preview

O1 preview is OpenAI's latest ChatGPT model. The main draw of this model is that it spends more time "thinking" or "reasoning" on more complex questions, resulting in a greater ability to respond to complex problems.
The human brain commits more thought to more difficult problems. We spend more time thinking if asked to solve a complex math problem than if asked what the weather is today, or what time it is. The o1 models by OpenAI
aim to replicate this fact, and do so quite successfully.

As per OpenAI's own evaluations, o1 ranks in the 89th percentile on competitive programming questions (Codeforces), places among the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME), 
and exceeds human PhD-level accuracy on a benchmark of physics, biology, and chemistry problems (GPQA). For comparison, in a qualifying exam for the International Mathematics Olympiad (IMO),
GPT-4o correctly solved only 13% of problems, while the reasoning model (o1) scored 83%.

Everyone tends to have models they use for different tasks, and there are many models to choose from. Anthropic's Claude 3.5 Sonnet, Google's Gemini, Meta's Llama 3, and Mystral's Mixtral 8x7B and 8x22B to name a few.
O1 is relatively new. That said, I think it shows great promise for use by scientists, given its capacity for reasoning.

A reddit user Sunil Kumar Dash reviewed the o1-preview model. His [reddit review](https://www.reddit.com/r/LocalLLaMA/comments/1ficb0z/o1preview_a_model_great_at_math_and_reasonong/) and [detailed review](https://composio.dev/blog/openai-o1-preview-a-detailed-analysis/) can be found by following both links. In his reviews, he confirms o1-previews reasoning and mathematical capacity, 
but seems unimpressed by its coding capabilities. I have used o1-preview to generate some code blocks for data analysis and had no issues, though I am working at a far lower complexity than a lot of software
experts would be working. That said, in the comments of his reddit post, user Jimin P suggests that o1-previews coding and logic abilities are greater than Sunil's estimations. Ultimately, I think most people using
this prompt library probably don't do very complicated coding, and so o1-preview should be an excellent model to use. Everyone's mileage may vary, so use the model that you like and works best for you. These
prompts and tutorials should be LLM agnostic.
